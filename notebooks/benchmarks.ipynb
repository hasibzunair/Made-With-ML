{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb04d61-c097-45a9-9201-e3649cbdc0cc",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\n",
    "    <h3>ML for Developers</h3>\n",
    "    Design Â· Develop Â· Deploy Â· Iterate\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <a target=\"_blank\" href=\"https://madewithml.com\"><img src=\"https://img.shields.io/badge/Subscribe-40K-brightgreen\"></a>&nbsp;\n",
    "    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/MadeWithML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/MadeWithML.svg?style=social&label=Star\"></a>&nbsp;\n",
    "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
    "    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n",
    "    <br>\n",
    "    ðŸ”¥&nbsp; Among the <a href=\"https://github.com/GokuMohandas/MadeWithML\" target=\"_blank\">top ML</a> repositories on GitHub\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3012ac29-11de-458c-9023-1a216812f943",
   "metadata": {},
   "source": [
    "# Generative AI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dffe7be-4a8f-499a-9531-fa4bbab228ef",
   "metadata": {},
   "source": [
    "In our [Made With ML course](https://madewithml.com/) we will be fine-tuning an LLM for a supervised classification task. The specific class of LLMs we'll be using is called [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)). Bert models are encoder-only models and are the gold-standard for supervised NLP tasks. However, you may be wondering how do all the (much larger) LLM, created for generative applications, fare ([GPT 4](https://openai.com/research/gpt-4), [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b), [Llama 2](https://ai.meta.com/llama/), etc.)?\n",
    "\n",
    "> We chose the smaller BERT model for our course because it's easier to train and fine-tune. However, the workflow for fine-tuning the larger LLMs are quite similar as well. They do require much more compute but Ray abstracts away the scaling complexities involved with that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d877530b-f3c5-429a-8525-1238c5b8693b",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c96931-d511-4c6e-b582-87d24455a11e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai==0.27.8 tqdm==4.65.0 -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9fb2cc9",
   "metadata": {},
   "source": [
    "You'll need to first sign up for an [OpenAI account](https://platform.openai.com/signup) and then grab your API key from [here](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a577e-3cd0-4c6b-81f9-8bc32850214d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d40cf2d-afa2-41b6-8f03-77b50cc3baca",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790e2f5-6b8b-425c-8842-a2b0ea8f3f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bfadb-ba49-4f5a-b216-4db14c8888ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "DATASET_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n",
    "train_df = pd.read_csv(DATASET_LOC)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b95d5-d61e-48e4-9100-d9d2fc0d53fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unique labels\n",
    "tags = train_df.tag.unique().tolist()\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c828129-8248-4e38-93a4-cabb097e7ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load inference dataset\n",
    "HOLDOUT_LOC = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\n",
    "test_df = pd.read_csv(HOLDOUT_LOC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "085dd167-0bee-4167-b1b7-d5797ebda30b",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5686dd79-02bc-4f11-8948-888b58bd504c",
   "metadata": {},
   "source": [
    "We'll define a few utility functions to make the OpenAI request and to store our predictions. While we could perform batch prediction by loading samples until the context length is reached, we'll just perform one at a time since it's not too many data points and we can have fully deterministic behavior (if you insert new data, etc.). We'll also added some reliability in case we overload the endpoints with too many request at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c3f44-2c19-4c32-9bc5-e9a7a917d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query OpenAI endpoint\n",
    "system_content = \"you only answer in rhymes\"  # system content (behavior)\n",
    "assistant_content = \"\"  # assistant content (context)\n",
    "user_content = \"how are you\"  # user content (message)\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    ")\n",
    "print (response.to_dict()[\"choices\"][0].to_dict()[\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "175dddcc",
   "metadata": {},
   "source": [
    "Now let's create a function that can predict tags for a given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aae14c-9870-4a27-b5ad-90f339686620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tag(model, system_content=\"\", assistant_content=\"\", user_content=\"\"):\n",
    "    try:\n",
    "        # Get response from OpenAI\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "        )\n",
    "        predicted_tag = response.to_dict()[\"choices\"][0].to_dict()[\"message\"][\"content\"]\n",
    "        return predicted_tag\n",
    "\n",
    "    except (openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tag\n",
    "model = \"gpt-3.5-turbo-0613\"\n",
    "system_context = f\"\"\"\n",
    "    You are a NLP prediction service that predicts the label given an input's title and description.\n",
    "    You must choose between one of the following labels for each input: {tags}.\n",
    "    Only respond with the label name and nothing else.\n",
    "    \"\"\"\n",
    "assistant_content = \"\"\n",
    "user_context = \"Transfer learning with transformers: Using transformers for transfer learning on text classification tasks.\"\n",
    "tag = get_tag(model=model, system_content=system_context, assistant_content=assistant_content, user_content=user_context)\n",
    "print (tag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3087ead2",
   "metadata": {},
   "source": [
    "Next, let's create a function that can predict tags for a list of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c43e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dicts w/ {title, description} (just the first 3 samples for now)\n",
    "samples = test_df[[\"title\", \"description\"]].to_dict(orient=\"records\")[:3]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9359a91-ac19-48a4-babb-e65d53f39b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(inputs, model, system_content, assistant_content=\"\"):\n",
    "    y_pred = []\n",
    "    for item in tqdm(inputs):\n",
    "        # Convert item dict to string\n",
    "        user_content = str(item)\n",
    "\n",
    "        # Get prediction\n",
    "        predicted_tag = get_tag(\n",
    "            model=model, system_content=system_content,\n",
    "            assistant_content=assistant_content, user_content=user_content)\n",
    "\n",
    "        # If error, try again after pause (repeatedly until success)\n",
    "        while predicted_tag is None:\n",
    "            time.sleep(30)  # could also do exponential backoff\n",
    "            predicted_tag = get_tag(\n",
    "                model=model, system_content=system_content,\n",
    "                assistant_content=assistant_content, user_content=user_content)\n",
    "\n",
    "        # Add to list of predictions\n",
    "        y_pred.append(predicted_tag)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for a list of inputs\n",
    "get_predictions(inputs=samples, model=model, system_content=system_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "190e7e72",
   "metadata": {},
   "source": [
    "Next we'll define a function that can clean our predictions in the event that it's not the proper format or has hallucinated a tag outside of our expected tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb38a8-44cb-4cea-828c-590f223d4063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_predictions(y_pred, tags, default=\"other\"):\n",
    "    for i, item in enumerate(y_pred):\n",
    "        if item not in tags:  # hallucinations\n",
    "            y_pred[i] = default\n",
    "        if item.startswith(\"'\") and item.endswith(\"'\"):  # GPT 4 likes to places quotes\n",
    "            y_pred[i] = item[1:-1]\n",
    "    return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6f18bd5",
   "metadata": {},
   "source": [
    "> Open AI has now released [function calling](https://openai.com/blog/function-calling-and-other-api-updates) and [custom instructions](https://openai.com/blog/custom-instructions-for-chatgpt) which is worth exploring to avoid this manual cleaning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f361ee27",
   "metadata": {},
   "source": [
    "Next, we'll define a function that will plot our ground truth labels and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tag_dist(y_true, y_pred):\n",
    "    # Distribution of tags\n",
    "    true_tag_freq = dict(Counter(y_true))\n",
    "    pred_tag_freq = dict(Counter(y_pred))\n",
    "    df_true = pd.DataFrame({\"tag\": list(true_tag_freq.keys()), \"freq\": list(true_tag_freq.values()), \"source\": \"true\"})\n",
    "    df_pred = pd.DataFrame({\"tag\": list(pred_tag_freq.keys()), \"freq\": list(pred_tag_freq.values()), \"source\": \"pred\"})\n",
    "    df = pd.concat([df_true, df_pred], ignore_index=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title(\"Tag distribution\", fontsize=14)\n",
    "    ax = sns.barplot(x=\"tag\", y=\"freq\", hue=\"source\", data=df)\n",
    "    ax.set_xticklabels(list(true_tag_freq.keys()), rotation=0, fontsize=8)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fc0b6ad",
   "metadata": {},
   "source": [
    "And finally, we'll define a function that will combine all the utilities above to predict, clean and plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_df, model, system_content, assistant_content, tags):\n",
    "    # Predictions\n",
    "    y_test = test_df.tag.to_list()\n",
    "    test_samples = test_df[[\"title\", \"description\"]].to_dict(orient=\"records\")\n",
    "    y_pred = get_predictions(\n",
    "        inputs=test_samples, model=model,\n",
    "        system_content=system_content, assistant_content=assistant_content)\n",
    "    y_pred = clean_predictions(y_pred=y_pred, tags=tags)\n",
    "\n",
    "    # Performance\n",
    "    metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "    performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n",
    "    print(json.dumps(performance, indent=2))\n",
    "    plot_tag_dist(y_true=y_test, y_pred=y_pred)\n",
    "    return y_pred, performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fb7ab2a",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83f00073",
   "metadata": {},
   "source": [
    "Now we're ready to start benchmarking our different LLMs with different context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fee2f-86e2-445e-92d0-923f5690132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = {\"zero_shot\": {}, \"few_shot\": {}}\n",
    "performance = {\"zero_shot\": {}, \"few_shot\": {}}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2029fff2-ae81-4cef-bef5-3cac717d0222",
   "metadata": {},
   "source": [
    "### Zero-shot learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "341b9d90",
   "metadata": {},
   "source": [
    "We'll start with zero-shot learning which involves providing the model with the `system_content` that tells it how to behave but no examples of the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4e745-ef56-4b76-8230-fcbe56ac46aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_content = f\"\"\"\n",
    "    You are a NLP prediction service that predicts the label given an input's title and description. \n",
    "    You must choose between one of the following labels for each input: {tags}. \n",
    "    Only respond with the label name and nothing else.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73780054-afeb-4ce6-8255-51bf91f9f820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zero-shot with GPT 3.5\n",
    "method = \"zero_shot\"\n",
    "model = \"gpt-3.5-turbo-0613\"\n",
    "y_pred[method][model], performance[method][model] = evaluate(\n",
    "    test_df=test_df, model=model, system_content=system_content,\n",
    "    assistant_content=\"\", tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af6d04-d29e-4adb-a289-4c34c2cc7ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zero-shot with GPT 4\n",
    "method = \"zero_shot\"\n",
    "model = \"gpt-4-0613\"\n",
    "y_pred[method][model], performance[method][model] = evaluate(\n",
    "    test_df=test_df, model=model, system_content=system_content,\n",
    "    assistant_content=\"\", tags=tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "483f6d46-7a9e-4bce-a34f-96c1cf2df29a",
   "metadata": {},
   "source": [
    "### Few-shot learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d6159b2",
   "metadata": {},
   "source": [
    "Now, we'll be adding a `assistant_context` with a few samples from our training data for each class. The intuition here is that we're giving the model a few examples (few-shot learning) of what each class looks like so that it can learn to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ed1e1-b34d-43d1-ae8b-32b1fd5be53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create additional context with few samples from each class\n",
    "num_samples = 2\n",
    "additional_context = []\n",
    "cols_to_keep = [\"title\", \"description\", \"tag\"]\n",
    "for tag in tags:\n",
    "    samples = train_df[cols_to_keep][train_df.tag == tag][:num_samples].to_dict(orient=\"records\")\n",
    "    additional_context.extend(samples)\n",
    "additional_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294548a5-9edf-4dea-ab8d-dc7464246810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add additional context\n",
    "assistant_content = f\"\"\"Here are some examples with the correct labels: {additional_context}\"\"\"\n",
    "print (assistant_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a087e14f",
   "metadata": {},
   "source": [
    "> We could increase the number of samples by increasing the context length. We could also retrieve better few-shot samples by extracting examples from the training data that are similar to the current sample (ex. similar unique vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bca273-3ea8-4ce0-9fa9-fe19062b7c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Few-shot with GPT 3.5\n",
    "method = \"few_shot\"\n",
    "model = \"gpt-3.5-turbo-0613\"\n",
    "y_pred[method][model], performance[method][model] = evaluate(\n",
    "    test_df=test_df, model=model, system_content=system_content,\n",
    "    assistant_content=assistant_content, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59a3b9-69d9-4bb5-8b88-0569fcc72f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Few-shot with GPT 4\n",
    "method = \"few_shot\"\n",
    "model = \"gpt-4-0613\"\n",
    "y_pred[method][model], performance[method][model] = evaluate(\n",
    "    test_df=test_df, model=model, system_content=system_content,\n",
    "    assistant_content=assistant_content, tags=tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61521970",
   "metadata": {},
   "source": [
    "As we can see, few shot learning performs better than it's respective zero shot counter part. GPT 4 has had considerable improvements in reducing hallucinations but for our supervised task this comes at an expense of high precision but lower recall and f1 scores. When GPT 4 is not confident, it would rather predict `other`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e04b53bf",
   "metadata": {},
   "source": [
    "## OSS LLMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6c72add",
   "metadata": {},
   "source": [
    "So far, we've only been using closed-source models from OpenAI. While these are *currently* the gold-standard, there are many open-source models that are rapidly catching up ([Falcon 40B](https://huggingface.co/tiiuae/falcon-40b), [Llama 2](https://ai.meta.com/llama/), etc.). Before we see how these models perform on our task, let's first consider a few reasons why we should care about open-source models.\n",
    "\n",
    "- **data ownership**: you can serve your models and pass data to your models, without having to share it with a third-party API endpoint.\n",
    "- **fine-tune**: with access to our model's weights, we can actually fine-tune them, as opposed to experimenting with fickle prompting strategies.\n",
    "- **optimization**: we have full freedom to optimize our deployed models for inference (ex. quantization, pruning, etc.) to reduce costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coming soon in August!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3724d63b-58f8-4374-a89d-275a83c8190e",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b498a-97c1-488c-a6b9-dc63a8a9df4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(performance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc80311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data into a new dictionary with four keys\n",
    "by_model_and_context = {}\n",
    "for context_type, models_data in performance.items():\n",
    "    for model, metrics in models_data.items():\n",
    "        key = f\"{model}_{context_type}\"\n",
    "        by_model_and_context[key] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the model names and the metric values\n",
    "models = list(by_model_and_context.keys())\n",
    "metrics = list(by_model_and_context[models[0]].keys())\n",
    "\n",
    "# Plotting the bar chart with metric scores on top of each bar\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "width = 0.2\n",
    "x = range(len(models))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_values = [by_model_and_context[model][metric] for model in models]\n",
    "    ax.bar([pos + width * i for pos in x], metric_values, width, label=metric)\n",
    "    # Displaying the metric scores on top of each bar\n",
    "    for pos, val in zip(x, metric_values):\n",
    "        ax.text(pos + width * i, val, f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xticks([pos + width for pos in x])\n",
    "ax.set_xticklabels(models, rotation=0, ha='center', fontsize=8)\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('GPT Benchmarks')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeb8c5ba-c22f-4fae-b63c-d13f8c23ade7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our best model is GPT 4 with few shot learning at an f1 score of ~92%. We will see in the [Made With ML course](https://madewithml.com/) how fine-tuning an LLM with a proper training dataset to change the actual weights of the last N layers (as opposed to the hard prompt tuning here) will yield similar/slightly better results to GPT 4 (at a fraction of the model size and inference costs).\n",
    "\n",
    "However, the best system might actually be a combination of using these few-shot hard prompt LLMs alongside fine-tuned LLMs. For example, our fine-tuned LLMs in the course will perform well when the test data is similar to the training data (similar distributions of vocabulary, etc.) but may not perform well on out of distribution. Whereas, these hard prompted LLMs, by themselves or augmented with additional context (ex. arXiv plugins in our case), could be used when our primary fine-tuned model is not so confident."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
